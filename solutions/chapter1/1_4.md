### Q: Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?

A:
Calling the first probability (not learning from exploratory moves) set P1 and second one (learning from exploratory moves) P2
conceptual sets?. It might be better to learn P2 because it will find new ways to win faster. P2 might result in more wins.
