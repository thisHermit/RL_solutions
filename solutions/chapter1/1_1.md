### Q: Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?

A:
Assuming the same policy is used for both agents and a non-trivial constant alpha,

The algorithm above is using temporal difference learning with
the update step like:

V(S<sub>t</sub>) <- V(S<sub>t</sub>) + _alpha_ [ V(S<sub>t + 1</sub>) - V(S<sub>t</sub>) ]

Now because don't account for the player being 'x' or 'o', there will be an asymmetric advantage given to the player wrt to which the policy is updated.

Assuming the policy to be calculated wrt the player 'x', the following board state:

x | x | x
| o | o
| |

has the value 1, but is 0 wrt to the other player. So if we use the exact same policy for the player 'o', the algorithm will learn an incorrect policy because player 'o' is actually _helping_ player 'x', and will therefore the learnt policy will also be different.

In case we account for this in the state (by adding another dimension representing the player), the policy learnt would probably be the exact same.
